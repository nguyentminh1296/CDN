{"paragraphs":[{"text":"import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\n\ncase class LogRecord( clientIp: String, clientIdentity: String, user: String, dateTime: String, request:String,statusCode:Int, bytesSent:Long, referer:String, userAgent:String )\ncase class Log(clientIp: String, dateTime: String, reqest:String, referer: String)\n\nval PATTERN = \"\"\"^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\S+) \"(\\S+)\" \"([^\"]*)\"\"\"\".r\ndef parseLogLine(log: String): LogRecord = {\n      try {\n        val res = PATTERN.findFirstMatchIn(log)\n \n        if (res.isEmpty) {\n          //println(\"Rejected Log Line: \" + log)\n          LogRecord(\"Empty\", \"-\", \"-\", \"\", \"\",  -1, -1, \"-\", \"-\" )\n        }\n        else {\n          val m = res.get\n          // NOTE:   HEAD does not have a content size.\n          if (m.group(9).equals(\"-\")) {\n            LogRecord(m.group(1), m.group(2), m.group(3), m.group(4),\n              m.group(6), m.group(8).toInt, 0, m.group(10), m.group(11))\n          }\n          else {\n            LogRecord(m.group(1), m.group(2), m.group(3), m.group(4),\n              m.group(6), m.group(8).toInt, m.group(9).toLong, m.group(10), m.group(11))\n          }\n        }\n      } catch\n      {\n        case e: Exception =>\n          //println(\"Exception on line:\" + log + \":\" + e.getMessage);\n          LogRecord(\"Empty\", \"-\", \"-\", \"\", \"-\", -1, -1, \"-\", \"-\" )\n      }\n    }\n\n    val logFile =  sc.textFile(\"/home/tranhoangvinh/Desktop/hcmut_access_2017.txt\")\n\n    val accesslog = logFile.map(parseLogLine).filter(!_.clientIp.equals(\"Empty\"))\n    \n    val log_full = accesslog.map(x => Log(x.asInstanceOf[LogRecord].clientIp, x.asInstanceOf[LogRecord].dateTime.slice(0,20), x.asInstanceOf[LogRecord].request, x.asInstanceOf[LogRecord].referer))\n    \n    val output = log_full.toDF().withColumn(\"timestamp\", unix_timestamp($\"dateTime\",\"dd/MMM/yyyy:HH:mm:ss\").cast(\"timestamp\")).drop(\"dateTime\").orderBy($\"clientIp\")\n    \n    //var count = 0\n    \n    val List_ip = output.select(\"clientIp\").collect().toList.distinct.map(x=> x.get(0))\n    \n    var List_ip_filter = List_ip.map(x =>  output.filter(y => y.getString(0).equals(x)))\n\n    val duration = List_ip_filter.map(x => {  \n        //count =  count + 1; \n        val names = Window.partitionBy('clientIp).orderBy('timestamp); \n        val withPreviousDateTime =  x.withColumn(\"previousTime\", lag('timestamp, 1) over names).withColumn(\"duration\",unix_timestamp('timestamp) - unix_timestamp('previousTime)) ; \n        //withPreviousDateTime.registerTempTable(\"data_full_\" + count.toString);\n        withPreviousDateTime;\n    })\n    \n    \n    duration.map(x => {\n        var hit = 0\n        val client_ip = x.map(y => y(0).asInstanceOf[String]).collect.head.toString\n        val array_refer = x.map(y => y(2).asInstanceOf[String]).collect\n        val array_duration = x.map(y => y(5).asInstanceOf[Long]).collect\n        if (array_refer.length == 1) \n            hit = hit + 1;\n        else {\n            for (i<-0 to array_refer.length -2 ){ \n                if(array_refer(i) != array_refer(i+1)) {\n                    if(i == array_refer.length - 2)\n                        hit = hit + 2\n                    else\n                        hit = hit + 1\n                }\n                else {\n                    if(array_duration(i+1) > 2)\n                        hit = hit + 1\n                }\n            } \n        }\n        println((client_ip,hit).toString)\n    })\n\n\n    println(count)\n    \n    \n     \n    //output.coalesce(1,true).saveAsTextFile(\"/home/tranhoangvinh/Desktop/output_test\")\n\n","user":"anonymous","dateUpdated":"2018-04-11T17:20:15+0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\nimport spark.implicits._\ndefined class LogRecord\ndefined class Log\nPATTERN: scala.util.matching.Regex = ^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\S+) \"(\\S+)\" \"([^\"]*)\"\nparseLogLine: (log: String)LogRecord\nlogFile: org.apache.spark.rdd.RDD[String] = /home/tranhoangvinh/Desktop/hcmut_access_2017.txt MapPartitionsRDD[15] at textFile at <console>:42\naccesslog: org.apache.spark.rdd.RDD[LogRecord] = MapPartitionsRDD[17] at filter at <console>:50\nlog_full: org.apache.spark.rdd.RDD[Log] = MapPartitionsRDD[18] at map at <console>:54\noutput: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [clientIp: string, reqest: string ... 2 more fields]\nList_ip: List[Any] = List(1.102.0.67, 1.46.198.53, 1.52.100.104, 1.52.107.182, 1.52.110.202, 1.52.120.141, 1.52.120.62, 1.52.123.208, 1.52.123.57, 1.52.131.84, 1.52.133.27, 1.52.140.34, 1.52.141.150, 1.52.141.205, 1.52.141.224, 1.52.157.161, 1.52.168.111, 1.52.177.189, 1.52.18.239, 1.52.196.233, 1.52.2.97, 1.52.207.243, 1.52.207.245, 1.52.224.31, 1.52.225.243, 1.52.230.221, 1.52.230.79, 1.52.231.98, 1.52.237.18, 1.52.237.201, 1.52.237.207, 1.52.237.223, 1.52.248.187, 1.52.248.75, 1.52.252.135, 1.52.252.173, 1.52.254.194, 1.52.30.101, 1.52.30.112, 1.52.32.158, 1.52.32.180, 1.52.32.243, 1.52.32.55, 1.52.32.85, 1.52.33.134, 1.52.33.16, 1.52.33.62, 1.52.34.148, 1.52.34.16, 1.52.34.179, 1.52.34.19, 1.52.34.20, 1.52.34.232, 1.52.34.244, 1.52.34.65, 1.52.34.91, 1.52.35.218, 1.52.35.24, 1.52.35...List_ip_filter: List[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = List([clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: string, reqest: string ... 2 more fields], [clientIp: ...duration: List[org.apache.spark.sql.DataFrame] = List([clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 more fields], [clientIp: string, reqest: string ... 4 m...(1.102.0.67,8)\n(1.46.198.53,9)\n(1.52.100.104,5)\n(1.52.107.182,19)\n(1.52.110.202,15)\n(1.52.120.141,1)\n(1.52.120.62,1)\n(1.52.123.208,2)\n(1.52.123.57,14)\n(1.52.131.84,0)\n(1.52.133.27,10)\n(1.52.140.34,18)\n(1.52.141.150,29)\n(1.52.141.205,4)\n(1.52.141.224,6)\n(1.52.157.161,1)\n(1.52.168.111,11)\n(1.52.177.189,1)\n(1.52.18.239,1)\n(1.52.196.233,10)\n(1.52.2.97,1)\n(1.52.207.243,1)\n(1.52.207.245,11)\n(1.52.224.31,16)\n(1.52.225.243,5)\n(1.52.230.221,5)\n(1.52.230.79,1)\n(1.52.231.98,1)\n(1.52.237.18,2)\n(1.52.237.201,1)\n(1.52.237.207,8)\n(1.52.237.223,28)\n(1.52.248.187,8)\n(1.52.248.75,14)\norg.apache.spark.SparkException: Job 211 cancelled part of cancelled job group zeppelin-20180410-091115_1959964440\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1625)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)\n  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2375)\n  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2351)\n  at $anonfun$1.apply(<console>:72)\n  at $anonfun$1.apply(<console>:69)\n  at scala.collection.immutable.List.map(List.scala:277)\n  ... 54 elided\n"}]},"apps":[],"jobName":"paragraph_1523326275113_-569660918","id":"20180410-091115_1959964440","dateCreated":"2018-04-10T09:11:15+0700","dateStarted":"2018-04-11T16:25:15+0700","dateFinished":"2018-04-11T16:44:35+0700","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:261"},{"text":"%sql\n\nSelect * from data_full_6\n","user":"anonymous","dateUpdated":"2018-04-11T16:29:55+0700","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":274,"optionOpen":false,"setting":{"stackedAreaChart":{"style":"stack"}},"commonSetting":{},"keys":[{"name":"clientIp","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"reqest","index":1,"aggr":"sum"}]},"helium":{}},"1":{"graph":{"mode":"table","height":93,"optionOpen":false}}},"editorSetting":{"language":"sql","editOnDblClick":false},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Table or view not found: data_full_6; line 1 pos 14\nset zeppelin.spark.sql.stacktrace = true to see full stacktrace"}]},"apps":[],"jobName":"paragraph_1523326482016_-180220054","id":"20180410-091442_1338892399","dateCreated":"2018-04-10T09:14:42+0700","dateStarted":"2018-04-11T16:29:55+0700","dateFinished":"2018-04-11T16:44:35+0700","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:262"},{"text":"%sql\n","user":"anonymous","dateUpdated":"2018-04-10T10:24:10+0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1523330650249_-1781985042","id":"20180410-102410_1216224571","dateCreated":"2018-04-10T10:24:10+0700","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:263"}],"name":"filter time and refer","id":"2DC3195ZT","angularObjects":{"2D8JYZUAW:shared_process":[],"2D9QHP846:shared_process":[],"2D7QVQQXK:shared_process":[],"2D8XEHD7Y:shared_process":[],"2DA9CACMT:shared_process":[],"2D833R4QT:shared_process":[],"2D8SQ5DB8:shared_process":[],"2D9UKMTM5:shared_process":[],"2DA9SKS2Y:shared_process":[],"2D7R5DMNF:shared_process":[],"2D8C4X116:shared_process":[],"2D926N6T3:shared_process":[],"2D9AFXHSN:shared_process":[],"2D7Y1QX23:shared_process":[],"2D995HA9M:shared_process":[],"2D9PUQQQA:shared_process":[],"2DA3FWZ7U:shared_process":[],"2DATS555J:shared_process":[],"2D94RPZCV:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}