
=============================================&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&=====================================================

/// Script sbt: ip time content

import org.apache.spark._

object Sample{

case class LogRecord( clientIp: String, clientIdentity: String, user: String, dateTime: String, request:String,statusCode:Int, bytesSent:Long, referer:String, userAgent:String )

val PATTERN = """^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+) (\S+)" (\d{3}) (\S+) "(\S+)" "([^"]*)"""".r

def parseLogLine(log: String): LogRecord = {
      try {
        val res = PATTERN.findFirstMatchIn(log)
 
        if (res.isEmpty) {
          //println("Rejected Log Line: " + log)
          LogRecord("Empty", "-", "-", "", "",  -1, -1, "-", "-" )
        }
        else {
          val m = res.get
          // NOTE:   HEAD does not have a content size.
          if (m.group(9).equals("-")) {
            LogRecord(m.group(1), m.group(2), m.group(3), m.group(4),
              m.group(6), m.group(8).toInt, 0, m.group(10), m.group(11))
          }
          else {
            LogRecord(m.group(1), m.group(2), m.group(3), m.group(4),
              m.group(6), m.group(8).toInt, m.group(9).toLong, m.group(10), m.group(11))
          }
        }
      } catch
      {
        case e: Exception =>
          //println("Exception on line:" + log + ":" + e.getMessage);
          LogRecord("Empty", "-", "-", "", "-", -1, -1, "-", "-" )
      }
    }

  
 
  //// Main Spark Program
  def main(args: Array[String]) {
  
 	val conf = new SparkConf()
	conf.set("spark.master","local")
	conf.set("spark.app.name","Sample")
	val sc = new SparkContext(conf)

     
    	val logFile =  sc.textFile("/home/ntm/Desktop/access_log/access.txt")
	    println("===== Log Count LOG FILE : %s".format(logFile.count()))
    	val accesslog = logFile.map(parseLogLine).filter(!_.clientIp.equals("Empty"))

		
	    val output = accesslog.map(x => { x.asInstanceOf[LogRecord].clientIp + "," + x.asInstanceOf[LogRecord].dateTime.slice(0,20) + "," + x.asInstanceOf[LogRecord].request })

	    println("===== Log Count: %s".format(accesslog.count()))
      		


	output.coalesce(1,true).saveAsTextFile("/home/ntm/Desktop/output")
 
    
   sc.stop()
  }
}

=============================================&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&=====================================================

======================================################################################==================================================
///// Tag Color   

import org.apache.spark._
import java.io._
import scala.io.Source

object Sample{
 
  //// Main Spark Program
  def main(args: Array[String]) {
  
 	val conf = new SparkConf()
	conf.set("spark.master","local")
	conf.set("spark.app.name","Sample")
	val sc = new SparkContext(conf)

	val textFile = sc.textFile("/home/ntm/Desktop/access_log/hcmut_access_2017.txt")

	val regex = "/.+"

	val split_word = textFile.flatMap(line => line.split(" ").filter(x => x.matches(regex))).map(word => (word,1.toLong)).reduceByKey(_+_)

	val split = split_word.sortBy(x => -x._2)

	//val evalution = split_word.map(_._2).sum / textFile.count()

	val id_content_number = split.zipWithIndex().map{ case(line,count) => {(count+1).toString + "," + line }}

	val sepaRank = Array(22,4,21,100)

	var count =0;	
	var posMH = -1
	var posM = -1
	var posML = -1
	val total01 = sepaRank(0) + sepaRank(1)
	val total012 = sepaRank(0) + sepaRank(1) + sepaRank(2)
	val total0123 = sepaRank(0) + sepaRank(1) + sepaRank(2) + sepaRank(3)


	val output = id_content_number.flatMap(line => line.split("\n").map( x => {
				count  += 1;
				if ( count <= sepaRank(0))  x.concat(",1111");
				else if (count > sepaRank(0) && count <= total01) {
					posMH += 1
					var xx = posMH % 4
					xx match {
						case 0 => x.concat(",1110");
						case 1 => x.concat(",1101");
						case 2 => x.concat(",1011");
						case 3 => x.concat(",0111");			    
					}

				}	
				else if (count > total01 && count <= total012) {
					posM += 1
					var xx = posM % 6
					xx match {
						case 0 => x.concat(",1100");
						case 1 => x.concat(",1010");
						case 2 => x.concat(",1001");
						case 3 => x.concat(",0110");
						case 4 => x.concat(",0101");
						case 5 => x.concat(",0011");
					}

				}
				else if (count > total012 && count <= total0123) {
					posML += 1
					var xx = posML % 4
					xx match {
						case 0 => x.concat(",1000");
						case 1 => x.concat(",0100");
						case 2 => x.concat(",0010");
						case 3 => x.concat(",0001");			    
					}
				}								
				else  x.concat(",0000");		
			}))


	output.coalesce(1,true).saveAsTextFile("/home/ntm/Desktop/output11")
		
	   sc.stop()
  }
}

======================================################################################==================================================

============================================*************************************************=======================================================

/////////////%%%%%%%%%%%%%%5   group time  Ip, content  %%%%%%%%%%%%5\\\\\\\\\\\\\\\\\\\\\\


import org.apache.commons.io.IOUtils
import java.net.URL
import java.nio.charset.Charset
import org.apache.spark.sql.functions.unix_timestamp

// create case class Grouptime
case class Grouptime(ip:String, time :String, content:String)

val input = sc.textFile("/home/ntm/Desktop/access_log/ip_time_content")


val data_parse =  input.map(s => s.split(",")).map(s => Grouptime(s(0).toString,s(1).toString,s(2).toString))

// cast time(String) to timestamp
val data_frame = data_parse.toDF().withColumn("timestamp", unix_timestamp($"time","dd/MMM/yyyy:HH:mm:ss").cast("timestamp"))
data_frame.show()

// groupBy follow time asc
val data_full = data_frame.groupBy(window($"timestamp", "2 seconds").as("time_interval"))
    .agg(collect_list("ip"),collect_list("content"))
    .orderBy($"time_interval".asc)
    
data_full.show()
data_full.registerTempTable("data_full")



============================================*************************************************=======================================================

==================================\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\=======================================================


//// count ip (Content) trong 1 gio


import org.apache.commons.io.IOUtils
import java.net.URL
import java.nio.charset.Charset
import org.apache.spark.sql.functions.unix_timestamp

// create case class Grouptime
case class Grouptime(ip:String, time :String, content:String)

val input = sc.textFile("/home/ntm/Desktop/access_log/ip_time_content_5day.txt")

val data_parse =  input.map(s => s.split(",")).map(s => Grouptime(s(0).toString,s(1).toString,s(2).toString))

// cast time(String) to timestamp
val data_frame = data_parse.toDF().withColumn("timestamp", unix_timestamp($"time","dd/MMM/yyyy:HH:mm:ss").cast("timestamp"))
data_frame.show()

val data_1min = data_frame.filter("timestamp BETWEEN '2016-11-23 01:00:00' AND '2016-11-23 02:00:00' ")
val data = data_1min.rdd.map(row => row.toString())
val file = data.map(x =>x.replaceAll("[\\[\\]]",""))

%%%%%% Count ip trong 1 hour

val regex = "[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}"

val split_word = file.flatMap(line => line.split(",").filter(x => x.matches(regex))).map(word => (word,1.toLong)).reduceByKey(_+_)

 
val splitsort = split_word.sortBy(x => -x._2)
val x = splitsort.map { case (k, v) => s"$k,$v" }
x.coalesce(1,true).saveAsTextFile("/home/ntm/Desktop/output3")

case class IPCount(ip:String, count_ip:Long)

val logip = x.map(s => s.split(",")).map(
    s => IPCount(s(0), s(1).toLong)
    ).toDF()
logip.registerTempTable("logip")
   


//val xx = data_frame.filter(data_frame("timestamp").lt(lit("2016-11-24")))
//xx.show()
//val xxx= xx.filter(xx("timestamp").lt(lit("2016-11-24")))

// groupBy follow time asc
//val data_full = data_frame.groupBy(window($"timestamp", "1 minute").as("time_interval")).agg(collect_list("ip"),collect_list("content")).orderBy($"time_interval".asc)


%%%%%%% Count content trong 1 hour


val regex = "/.+"

val split_word = file.flatMap(line => line.split(",").filter(x => x.matches(regex))).map(word => (word,1.toLong)).reduceByKey(_+_)

 
val splitsort = split_word.sortBy(x => -x._2)
val x = splitsort.map { case (k, v) => s"$k,$v" }
val id_content_number = x.zipWithIndex().map{ case(line,count) => {(count+1).toString + "," + line }}
id_content_number.coalesce(1,true).saveAsTextFile("/home/ntm/Desktop/output4")

case class ContentCount(id:Long, content:String,count_id:Long)

val logip = id_content_number.map(s => s.split(",")).map(
    s => ContentCount(s(0).toLong, s(1).toString,s(2).toLong)
    ).toDF()
logip.registerTempTable("logContent")

==================================\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\=======================================================
===================================***************************************************==================================================

/**
 Loc luot truy cap cua  1 ip
**/


import org.apache.commons.io.IOUtils
import java.nio.charset.Charset
import org.apache.spark.sql.SQLContext._
import spark.implicits._
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._
import java.sql.Timestamp

 // ** lay 4 cot va sort theo ip ****///
 val logFile = sc.textFile("/home/ntm/Desktop/access_log/1_46_198_53")
 case class Log(ip:String, time :String, content:String, refer : String)
    val data_log  =  logFile.map(s => s.split(",")).map(s => Log(s(0).toString,s(1).toString,s(2).toString,s(3).toString))

    val data_frame = data_log.toDF()
	//data_frame.sort("ip","time")  /**  sort theo ip, time  **/
    val data    =    data_frame.withColumn("timestamp", unix_timestamp($"time","dd/MMM/yyyy:HH:mm:ss").cast("timestamp"))
     
    val  data_collect = data.collect();
    var i =0;
    var firstx = data_collect(0);
    for( i <- 0 to (data_collect.length -1) )
        {
            var row = data_collect(i);
                    if((i == 0) || (row(3) != firstx(3)) )
                         { 
                             println(row(3) + "  " + row(4));
                             firstx = row;
                         }
                    else if(row(3) == firstx(3))
                    {
						/* durationRDD   thoi gian truy cap trong bao lau*/
                        var durationRDD =(Timestamp.valueOf(row(4).toString).getTime - Timestamp.valueOf(firstx(4).toString).getTime)/1000;
                        if(durationRDD  > 10 )
                        {
                            println(row(3) + "   " + row(4));
                             firstx = row;
                        }
                        
                    }
        }
    


===================================***************************************************==================================================